{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e551a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('DA_Test').config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.7\").getOrCreate()\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, regexp_replace, date_format, to_timestamp, expr,from_unixtime,unix_timestamp,concat_ws, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5110a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_spark = spark.read \\\n",
    "#     .format(\"com.crealytics.spark.excel\") \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .load(\"\\\\\\\\Data\\\\\\\\Python_exercise1.xlsx\")\n",
    "\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"data/Python_exercise1.xlsx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd9afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+------+---------------+-----+-----------+-----------+-------+----------+--------------+------+-----------------+-------------------+----------------+--------------+------------------+----------------+------+----------------+--------+------+----------+--------+-------+-------+----------------+---------------+------------+------------+---------+---------+------------+-------+---------+--------+---------+------+---------+----------+----------+-----+----+-----------+----------------+----------------+---------------------+--------------------------------------------------------------+-------------------------------+\n",
      "|No |Time                 |Level |Firewall Action|User |Source     |Destination|Service|Sent Bytes|Received Bytes|Threat|Application      |Application Details|Application Type|Application ID|Application Action|Application List|Attack|Device ID       |Duration|Log ID|Session ID|Sub Type|Type   |Message|Destination Name|Security Action|UTM Severity|UTM Sub Type|UTM Event|Policy ID|Tran Display|Tran IP|Tran Port|Protocol|File Name|Sender|Recipient|Mail Count|Spam Count|Virus|VDom|Source Port|Destination Port|Source Interface|Destination Interface|ReporttoManager                                               |enName                         |\n",
      "+---+---------------------+------+---------------+-----+-----------+-----------+-------+----------+--------------+------+-----------------+-------------------+----------------+--------------+------------------+----------------+------+----------------+--------+------+----------+--------+-------+-------+----------------+---------------+------------+------------+---------+---------+------------+-------+---------+--------+---------+------+---------+----------+----------+-----+----+-----------+----------------+----------------+---------------------+--------------------------------------------------------------+-------------------------------+\n",
      "|1.0|1/8/2022  10.01.00 AM|notice|client-rst     |guest|192.168.2.4|13.107.4.50|HTTP   |2451.0    |9327.0        |null  |MS.Windows.Update|null               |unscanned       |null          |null              |null            |null  |FGT80E4Q17012201|180.0   |13.0  |2671211.0 |forward |traffic|null   |null            |null           |null        |null        |null     |2.0      |noop        |null   |null     |17.0    |null     |null  |null     |null      |null      |null |root|54532.0    |53.0            |WIFI-1          |Lan-vlan             |CN=d921478,OU=People,OU=eProfile,DC=core,DC=dir,DC=tdog,DC=com|wsaddc0005.core.dir.company.com|\n",
      "+---+---------------------+------+---------------+-----+-----------+-----------+-------+----------+--------------+------+-----------------+-------------------+----------------+--------------+------------------+----------------+------+----------------+--------+------+----------+--------+-------+-------+----------------+---------------+------------+------------+---------+---------+------------+-------+---------+--------+---------+------+---------+----------+----------+-----+----+-----------+----------------+----------------+---------------------+--------------------------------------------------------------+-------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64207f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp,trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11845cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Time_temp', regexp_replace(col('Time'), '\\.', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "810087bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"cleaned_text\", regexp_replace(\"Time_temp\", r\"\\s{2,}\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dd7260b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|cleaned_text        |Time_temp            |\n",
      "+--------------------+---------------------+\n",
      "|1/8/2022 10:02:00 AM|1/8/2022  10:02:00 AM|\n",
      "|1/8/2022 10:07:00 AM|1/8/2022  10:07:00 AM|\n",
      "|1/8/2022 10:05:00 AM|1/8/2022  10:05:00 AM|\n",
      "|1/8/2022 10:04:00 AM|1/8/2022  10:04:00 AM|\n",
      "|1/8/2022 10:06:00 AM|1/8/2022  10:06:00 AM|\n",
      "|1/8/2022 10:03:00 AM|1/8/2022  10:03:00 AM|\n",
      "|1/8/2022 10:01:00 AM|1/8/2022  10:01:00 AM|\n",
      "|1/8/2022 10:09:00 AM|1/8/2022  10:09:00 AM|\n",
      "|1/8/2022 10:13:00 AM|1/8/2022  10:13:00 AM|\n",
      "|1/8/2022 10:08:00 AM|1/8/2022  10:08:00 AM|\n",
      "+--------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"cleaned_text\",\"Time_temp\").distinct().show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e98720a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('No', 'double'),\n",
       " ('Time', 'string'),\n",
       " ('Level', 'string'),\n",
       " ('Firewall Action', 'string'),\n",
       " ('User', 'string'),\n",
       " ('Source', 'string'),\n",
       " ('Destination', 'string'),\n",
       " ('Service', 'string'),\n",
       " ('Sent Bytes', 'double'),\n",
       " ('Received Bytes', 'double'),\n",
       " ('Threat', 'string'),\n",
       " ('Application', 'string'),\n",
       " ('Application Details', 'string'),\n",
       " ('Application Type', 'string'),\n",
       " ('Application ID', 'double'),\n",
       " ('Application Action', 'string'),\n",
       " ('Application List', 'string'),\n",
       " ('Attack', 'string'),\n",
       " ('Device ID', 'string'),\n",
       " ('Duration', 'double'),\n",
       " ('Log ID', 'double'),\n",
       " ('Session ID', 'double'),\n",
       " ('Sub Type', 'string'),\n",
       " ('Type', 'string'),\n",
       " ('Message', 'string'),\n",
       " ('Destination Name', 'string'),\n",
       " ('Security Action', 'string'),\n",
       " ('UTM Severity', 'string'),\n",
       " ('UTM Sub Type', 'string'),\n",
       " ('UTM Event', 'string'),\n",
       " ('Policy ID', 'double'),\n",
       " ('Tran Display', 'string'),\n",
       " ('Tran IP', 'string'),\n",
       " ('Tran Port', 'double'),\n",
       " ('Protocol', 'double'),\n",
       " ('File Name', 'string'),\n",
       " ('Sender', 'string'),\n",
       " ('Recipient', 'string'),\n",
       " ('Mail Count', 'string'),\n",
       " ('Spam Count', 'string'),\n",
       " ('Virus', 'string'),\n",
       " ('VDom', 'string'),\n",
       " ('Source Port', 'double'),\n",
       " ('Destination Port', 'double'),\n",
       " ('Source Interface', 'string'),\n",
       " ('Destination Interface', 'string'),\n",
       " ('ReporttoManager', 'string'),\n",
       " ('enName', 'string'),\n",
       " ('timestamp_col', 'timestamp'),\n",
       " ('Time_temp', 'string'),\n",
       " ('cleaned_text', 'string')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3557b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---+-----+----+----+------+------+\n",
      "|Time_temp            |day|month|year|hour|minute|second|\n",
      "+---------------------+---+-----+----+----+------+------+\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "|1/8/2022  10:01:00 AM|1  |8    |2022|10  |01    |00    |\n",
      "+---------------------+---+-----+----+----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, concat, lpad, col,lit\n",
    "#from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, concat, lpad, col,lit,split,size\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "#spark = SparkSession.builder.appName(\"DateConversionWithRegex\").getOrCreate()\n",
    "\n",
    "# Extract components using regexp_extract\n",
    "df = df.withColumn(\"date\", split(df[\"Time_temp\"], \" \")[0])\n",
    "df = df.withColumn(\"time\", split(df[\"Time_temp\"], \"  \")[1])\n",
    "\n",
    "# Extract date components\n",
    "df = df.withColumn(\"day\", split(df[\"date\"], \"/\")[0].cast(\"int\"))\n",
    "df = df.withColumn(\"month\", split(df[\"date\"], \"/\")[1].cast(\"int\"))\n",
    "df = df.withColumn(\"year\", split(df[\"date\"], \"/\")[2].cast(\"int\"))\n",
    "\n",
    "\n",
    "\n",
    "df = df.withColumn(\"hour\", split(df[\"time\"], \":\")[0])\n",
    "\n",
    "df = df.withColumn(\"minute\", split(df[\"time\"], \":\")[1])\n",
    "\n",
    "df = df.withColumn(\"second\", split(df[\"time\"], \":\")[2].substr(1,2))\n",
    "\n",
    "\n",
    "# Show the results\n",
    "df.select(\"Time_temp\", \"day\",\"month\",\"year\",\"hour\",\"minute\",\"second\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25cd1c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|time_new           |\n",
      "+-------------------+\n",
      "|10/01/00 01/08/2022|\n",
      "|10/03/00 01/08/2022|\n",
      "|10/04/00 01/08/2022|\n",
      "|10/07/00 01/08/2022|\n",
      "|10/02/00 01/08/2022|\n",
      "|10/06/00 01/08/2022|\n",
      "|10/05/00 01/08/2022|\n",
      "|10/09/00 01/08/2022|\n",
      "|10/13/00 01/08/2022|\n",
      "|10/08/00 01/08/2022|\n",
      "|10/12/00 01/08/2022|\n",
      "|10/11/00 01/08/2022|\n",
      "|10/10/00 01/08/2022|\n",
      "|10/19/00 01/08/2022|\n",
      "|10/15/00 01/08/2022|\n",
      "|10/17/00 01/08/2022|\n",
      "|10/14/00 01/08/2022|\n",
      "|10/16/00 01/08/2022|\n",
      "|10/18/00 01/08/2022|\n",
      "|10/25/00 01/08/2022|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_string\n",
    "df = df.withColumn(\"d1\", format_string(\"%02d\", col(\"month\")) )\n",
    "df = df.withColumn(\"d2\", format_string(\"%02d\", col(\"day\")) )\n",
    "df = df.withColumn(\"time\", concat_ws(\"/\", df[\"hour\"], df[\"minute\"], df[\"second\"]))\n",
    "df =df.withColumn(\"date\", concat_ws(\"/\" , df[\"d2\"], df[\"d1\"], df[\"year\"]))\n",
    "\n",
    "df =df.withColumn(\"time_new\", concat_ws(\" \",df[\"time\"],df[\"date\"]))\n",
    "#Assignment Q1 answer \n",
    "df.select(\"time_new\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adad3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ebb2062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'date_format(to_timestamp(1/8/2022 10:01:00 AM, d/M/yyyy hh:mm:ss a), dd/MM/yyyy hh:mm:ss a)'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eeab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e03d80f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90f6112a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o551.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 44.0 failed 1 times, most recent failure: Lost task 4.0 in stage 44.0 (TID 145) (NBDM11C-048.prevalent.com executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '10/26/00 01/08/2022' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:918)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:117)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.time.format.DateTimeParseException: Text '10/26/00 01/08/2022' could not be parsed at index 12\r\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\r\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:109)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '10/26/00 01/08/2022' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:918)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:117)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.time.format.DateTimeParseException: Text '10/26/00 01/08/2022' could not be parsed at index 12\r\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\r\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:109)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime_temp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp_col\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:502\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncate=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be either bool or int.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(truncate))\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o551.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 44.0 failed 1 times, most recent failure: Lost task 4.0 in stage 44.0 (TID 145) (NBDM11C-048.prevalent.com executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '10/26/00 01/08/2022' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:918)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:117)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.time.format.DateTimeParseException: Text '10/26/00 01/08/2022' could not be parsed at index 12\r\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\r\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:109)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '10/26/00 01/08/2022' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseDateTimeInNewParserError(QueryExecutionErrors.scala:918)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:148)\r\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:117)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.time.format.DateTimeParseException: Text '10/26/00 01/08/2022' could not be parsed at index 12\r\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\r\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\r\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:109)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49b838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
